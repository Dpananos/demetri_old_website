[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Quarto Blog",
    "section": "",
    "text": "I’ve moved my blog to Quarto! It will take some time to migrate my old posts here so please be patient."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Blog About Statistics, Mathematics, and Other Stuff",
    "section": "",
    "text": "Statistics\n\n\nMachine Learning\n\n\nPython\n\n\nScikit-Learn\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJun 22, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2022\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2021\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nStatistics\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nProbability\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2018\n\n\nDemetri Pananos\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nRiddler\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2017\n\n\nDemetri Pananos\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "PhDemetri",
    "section": "",
    "text": "I’m a Senior Data Scientist at Zapier focusing on improving A/B Tests and experimentation."
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html",
    "href": "posts/2017-12-29-coins/index.html",
    "title": "Coins and Factors",
    "section": "",
    "text": "I love Fivethirtyeight’s Riddler column. Usually, I can solve the problem with computation, but on some rare occasions I can do some interesting math to get the solution without having to code. Here is the first puzzle I ever solved. It is a simple puzzle, yet it has an elegant computational and analytic solution. Let’s take a look.\nThe puzzle says:"
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html#computing-the-solution",
    "href": "posts/2017-12-29-coins/index.html#computing-the-solution",
    "title": "Coins and Factors",
    "section": "Computing the Solution",
    "text": "Computing the Solution\nThis is really easy to program. Here is a little python script to compute the solution:\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import product\n\n\n# Array of 100 True.  True is Heads up\nNcoins = 100\ncoins = np.ones(Ncoins,dtype = bool)\nindex = np.arange(1,Ncoins+1)\n\n#Go through the coins\nfor N in range(1,Ncoins+1):\n    \n    coins[index%N==0] = ~coins[index%N==0]  #Flip the coin.  \nShown below is the solution. In dark blue are the coins face down (I’ve arranged them in a 10 by 10 grid and annotated them with their position for clarity). When we take a look at the coins variable, we see that those coins in positions which are perfect squares pop out. That is an interesting result, but what is more interesting is reasoning out the solution without doing any computation at all!\n\n\n\n\n\nFigure 1: Coins after flipping. Dark squares are face down coins."
  },
  {
    "objectID": "posts/2017-12-29-coins/index.html#reasoning-out-the-solution.",
    "href": "posts/2017-12-29-coins/index.html#reasoning-out-the-solution.",
    "title": "Coins and Factors",
    "section": "Reasoning Out the Solution.",
    "text": "Reasoning Out the Solution.\nFirst, let’s think why a coin would end up face down. If all coins start heads up, then it would take an odd number of flips for the coin to end up face down. Since coins are only flipped when we pass a factor of a coin’s position, then those coins in positions with an odd number of factors will be heads down at the end.\nSo 9 would end up heads down because it has factors 1 (flip face down), 3 (flip face up), and 9 (flip face down), while 6 would be heads up because it has factors 1, 2, 3, and 6.\nSo which numbers have an odd number of factors? Here is where we get to do some interesting math. The Fundamental Theorem of Arithmetic says that every integer \\(N>1\\) is either prime or can be uniquely factored as a product of primes\n\\[N = \\prod_{j} p_j^{a_j} \\>.\\]\nIf \\(N\\) can be factored like this, that means it has\n\\[\\prod_{j} (a_j +1)\\]\nunique factors.\nIt is straight forward to argue that a composite odd number must be the product of odd numbers, so we know that the \\(a_j+1\\) must be odd \\(\\forall j\\), and so that means the \\(a_j\\) are even and can be written as \\(a_j = 2n_j\\). Thus, our factorization becomes\n\\[N = \\prod_j p_j^{2n_j} = \\prod_j (p_j^{n_j})^2 = \\left(\\prod_j p_j^{n_j} \\right)^2 \\>,\\]\nwhich means that if \\(N\\) has an odd number of factors, it must be a perfect square! All done.\nI love trying to solve the Riddler’s puzzle without coding. It makes me draw upon knowledge I haven’t used in a while, and may force me to learn something new."
  },
  {
    "objectID": "posts/2018-08-31-combinatorics/index.html",
    "href": "posts/2018-08-31-combinatorics/index.html",
    "title": "Neat Litle Combinatorics Problem",
    "section": "",
    "text": "We could always just sample form the set to estimate the expected value. Here is a python script to do just that.\n\nimport numpy as np\nx = np.array([49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46])\n\nmins = []\nfor _ in range(1000):\n    mins.append(np.random.choice(x,size = 6, replace = False).min())\n\nprint(np.mean(mins))\n\n9.098\n\n\nBut that is estimating the mean. We can do better and directly compute it. Here is some python code to create all subsets from \\(S\\) of size 6. Then, we simply take out the minimum from each subset and compute the mean.\n\nimport numpy as np\nfrom itertools import combinations, groupby\n\nx = np.array([49, 8, 48, 15, 47, 4, 16, 23, 43, 44, 42, 45, 46])\nx = np.sort(x)\n\nc = list(combinations(x,6))\n\nmins = list(map(lambda x: x[0], c))\n\ns = 0\nfor k, g in groupby(sorted(mins)):\n    s+=k*(len(list(g))/len(mins))\n\nprint( s )\n\n8.818181818181818\n\n\nThe script returns 8.18 repeating. Great, but we can do even better! If we can compute the probability density function, we can compute the mean analytically. Let’s consider a smaller problem to outline the solution.\nLet our set in question be \\((1,2,3,4,5)\\). Let the minimum of a sample of 3 numbers from this set be the random variable \\(z\\). Now, note there are \\(\\binom{5}{3} = 10\\) ways to choose 3 elements from a set of 5.\nHow many subsets exist where the minimum is 1? Well, if I sampled 1, then I would still have to pick 2 numbers from a possible 4 numbers larger than 1. There are \\(\\binom{4}{2}\\) ways to do this. So \\(p(z=1) = \\binom{4}{2} / \\binom{5}{3}\\).\nIn a similar fashion, there are \\(\\binom{3}{2}\\) subsets where 2 is the minimum, and \\(\\binom{2}{2}\\) subsets where 3 is the minimum. There are no subsets where 4 or 5 are the minimum (why?). So that means the expected minimum value for this set would be\n\\[\\operatorname{E}(z) = \\dfrac{ \\sum_{k = 1}^{3} k\\binom{5-k}{2} }{\\binom{5}{3}}  \\]\nWhatever that sum happens to be. Here is how you could code up the analytic solution to our problem.\n\nimport numpy as np\nfrom scipy.special import binom\n\nx = np.array([ 4, 8, 15, 16, 23, 42, 43, 44, 45, 46, 47, 48, 49])\nx = np.sort(x)\n\nsample_size =6\nsample_space = x[:-(sample_size-1)]\nE = 0\nfor i,s in enumerate(sample_space,start = 1):\n\n    E+= s*binom(x.size-i,sample_size-1)\n\nprint(E/binom(x.size, sample_size))\n\n8.818181818181818\n\n\nFull disclosure, this was on a job application (literally, on the job application), so sorry KiK for putting the answer out there, but the question was too fun not to write up!"
  },
  {
    "objectID": "posts/2020-10-06/index.html",
    "href": "posts/2020-10-06/index.html",
    "title": "Log Link vs. Log(y)",
    "section": "",
    "text": "set.seed(0)\nN = 1000\ny = rlnorm(N, 0.5, 0.5)\n\nand explain why glm(y ~ 1, family = gaussian(link=log) and lm(log(y)~1) produce different estimates of the coefficients. In case you don’t have an R terminal, here are the outputs\n\nlog_lm = lm(log(y) ~ 1)\nsummary(log_lm)\n\n\nCall:\nlm(formula = log(y) ~ 1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.61028 -0.34631 -0.02152  0.35173  1.64112 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.49209    0.01578   31.18   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.499 on 999 degrees of freedom\n\n\n\nglm_mod = glm(y ~ 1 , family = gaussian(link=log))\nsummary(glm_mod)\n\n\nCall:\nglm(formula = y ~ 1, family = gaussian(link = log))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5282  -0.6981  -0.2541   0.4702   6.5869  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.61791    0.01698    36.4   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.9918425)\n\n    Null deviance: 990.85  on 999  degrees of freedom\nResidual deviance: 990.85  on 999  degrees of freedom\nAIC: 2832.7\n\nNumber of Fisher Scoring iterations: 5\n\n\nAnswer is the same as the difference between \\(E(g(X))\\) and \\(g(E(X))\\) which are not always the same. Let me explain.\nFirst, let’s start with the lognormal random variable. \\(y \\sim \\operatorname{Lognormal}(\\mu, \\sigma)\\) means \\(\\log(y) \\sim \\operatorname{Normal}(\\mu, \\sigma)\\). So \\(\\mu, \\sigma\\) are the parameters of the underlying normal distribution. When we do lm(log(y) ~ 1), we are modelling \\(E(\\log(y)) = \\beta_0\\). So \\(\\beta_0\\) is an estimate of \\(\\mu\\) and \\(\\exp(\\mu)\\) is an estimate of the median of the lognormal. That is an easy check\n\nmedian(y)\n\n[1] 1.600898\n\n\n\n#Meh, close enough\nexp(coef(log_lm))\n\n(Intercept) \n   1.635723 \n\n\nIf I wanted an estimate of the mean of the lognormal, I would need to add \\(\\sigma^2/2\\) to my estimate of \\(\\mu\\).\n\nmean(y)\n\n[1] 1.855038\n\n\n\n#Meh, close enough\nsigma = var(log_lm$residuals)\nexp(coef(log_lm) + sigma/2)\n\n(Intercept) \n   1.852594 \n\n\nOk, onto the glm now. When we use the glm, we model \\(\\log(E(y)) = \\beta_0\\), so we model the mean of the lognormal directly. Case in point\n\nmean(y)\n\n[1] 1.855038\n\n\n\nexp(coef(glm_mod))\n\n(Intercept) \n   1.855038 \n\n\nand if I wanted the median, I would need to consider the extra factor of \\(\\exp(\\sigma^2/2)\\)\n\nmedian(y)\n\n[1] 1.600898\n\n\n\nexp(coef(glm_mod) - sigma/2)\n\n(Intercept) \n   1.637881 \n\n\nLog link vs. log outcome can be tricky. Just be sure to know what you’re modelling when you use either."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html",
    "href": "posts/2021-04-03-confidence-intervals/index.html",
    "title": "On Interpretations of Confidence Intervals",
    "section": "",
    "text": "The 95% in 95% confidence interval refers not to the probability that any one interval contains the estimand, but rather to the long term relative frequency of the estimator containing the estimand in an infinite sequence of replicated experiments under ideal conditions.\nNow, if this were twitter I would get ratioed so hard I might have to take a break and walk it off. Luckily, this is my blog and not yours so I can say whatever I want with impunity. But, rather than shout my opinions and demand people listen, I thought it might be a good exercise to explain to you why I think this and perhaps why people might disagree. Let’s for a moment ignore the fact that the interpretation I use above is the de jure definition of a confidence interval and instead start where a good proportion of statistical learning starts; with a deck of shuffled cards.\nI present to you a shuffled deck. Its a regular deck of cards, no funny business with the cards or the shuffling. What is the probability the top card of this deck an ace? I’d wager a good portion of people would say 4/52. If you, dear reader, said 4/52 then I believe you have made a benign mistake, but a mistake all the same. And I suspect the reason you’ve made this mistake is because you’ve swapped a hard question (the question about this deck) for an easier question (a question about the long term relative frequencies of coming to shuffled decks with no funny business and finding aces).\nSwapping hard questions for easy questions is not a new observation. Daniel Khaneman writes about it in Thinking Fast and Slow and provides numerous examples. I’ll repeat some examples from the book here. We might swap the question:\nThe book Thinking Fast and Slow explains why we do this, or better yet why we have no control over this. I won’t explain it here. But it is important to know that this is something we do, mostly unconsciously.\nSo back to the deck of cards. Questions about the deck in front of you are hard. Its either an ace or not, but you can’t tell! The card is face down and there is no other information you could use to make the decision. So, you answer an easier one using information that you do know, namely the number of aces in the deck, the number of cards in the deck, the information that each card is equally likely to be on top given the fact there is no funny business with the cards or the shuffling, and the basic rules of probability you might have learned in high school if not elsewhere. But the answer you give is for a fundamentally different question, namely “If I were to observe a long sequence of well shuffled decks with no funny business, what fraction of them have an ace on top?”. Your answer is about that long sequence of shuffled decks. It isn’t about any one particular deck, and certainly not the one in front of you.\nI think the same thing happens with confidence intervals. The estimator has the property that 95% of the time it is constructed (under ideal circumstances) it will contain the estimand. But any one interval does or does not contain the estimand. And unlike the deck of cards which can easily be examined, we can’t ever know for certain if the interval successfully captured the estimand. There is no moment where we get to verify the estimand is in the confidence interval, and so we are sort of left guessing thus prompting us to offer a probability that we are right.\nThe mistake is benign. It hurts no one to think about confidence intervals as having a 95% probability of containing the estimand. Your company will not lose money, your paper will (hopefully) not be rejected, and the world will not end. That being said, it is unfortunately incorrect if not by appealing to the definition, then perhaps for other reasons.\nI’ll start with an appeal to authority. Sander Greenland and coauthors (who include notable epidemiologist Ken Rothman and motherfucking Doug Altman) include interpretation of a confidence interval as having 95% probability of containing the true effect as misconception 19 in this amazing paper. They note ” It is possible to compute an interval that can be interpreted as having 95% probability of containing the true value” but go on to say that this results in us doing a Bayesian analysis and computing a credible interval. If these guys are wrong, I don’t want to be right.\nAdditionally, when I say “The probability of a coin being flipped heads is 0.5” that references a long term frequency. I could, in principle, demonstrate that frequency by flipping a coin a lot and computing the empirical frequency of heads, which assuming the coin is fair and the number of flips large enough, will be within an acceptable range 0.5. To those people who say “This interval contains the estimand with 95% probability” I say “prove it”. Demonstrate to me via simulation or otherwise this long term relative frequency. I can’t imagine how this could be demonstrated because any fixed dataset will yield same answer over and over. Perhaps what supporters of this perspective mean is something closer to the Bayesian interpretation of probability (where probability is akin to strength in a belief). If so, the debate is decidedly answered because probability in frequentism is not about belief strength but about frequencies. Additionally, what is the random component in this probability? The data from the experiment are fixed, to allow these to vary is to appeal to my interpretation of the interval. If the estimand is random, then we are in another realm all together as frequentism assumes fixed parameters and random data. Maybe they mean something else which I just can’t think of. If there is something else, please let me know.\nI’ve gotten flack about confidence intervals on twitter."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-1-framing-it-as-a-bet",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-1-framing-it-as-a-bet",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 1: Framing It As A Bet",
    "text": "Flack 1: Framing It As A Bet\nYou present to me a shuffled deck with no funny business and offer me a bet in which I win X0,000 dollars if the card is an ace and lose X0 dollars if the card is not. “Aha Demetri! If you think the probability of the card on top being an ace is 0 or 1 you are either a fool for not taking the bet or are a fool for being so over confident! Your position is indefensible!” one person on twitter said to me (ok, they didn’t say it verbatim like this, but that was the intent).\nWell, not so fast. Nothing about my interpretation precludes me from using the answer to a simpler question to make decisions (I would argue statistics is the practice of doing jus that, but I digress). The top card is still an ace or not, but I can still think about an infinite sequence of shuffled decks anyway. In most of those scenarios, the card on top is an ace. Thus, I take the bet and hope the top card is an ace (much like I hope the confidence interval captures the true estimand, even though I know it either does or does not)."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-2-my-next-interval-has-95-probability",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-2-my-next-interval-has-95-probability",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 2: My Next Interval Has 95% Probability",
    "text": "Flack 2: My Next Interval Has 95% Probability\n“But Demetri, if 95% refers to the frequency of intervals containing the estimand, then surely my next interval has 95% probability of capturing the estimand prior to seeing data. Hence, individual intervals do have 95% probability of containing the estimand”.\nI get this sometimes, but don’t fully understand how it is supposed to be convincing. I see no problem with saying “the next interval has 95% probability” just like I have no problem with saying “If you shuffle those cards, the probability an ace is on top is 4/52” or “My next Roll Up The Rim cup has a 1 in 6 chance of winning”. This is starting to get more philosophical than I care it to, but those all reference non-existent things. Once they are brought into existence, it would be silly to think that they retain these properties. My cup is either winner or loser, even if I don’t roll it."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#flack-3-but-schrödingers-cat",
    "href": "posts/2021-04-03-confidence-intervals/index.html#flack-3-but-schrödingers-cat",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Flack 3: But Schrödinger’s Cat…",
    "text": "Flack 3: But Schrödinger’s Cat…\nNo. Stop. This is not relevant in the least. I’m talking about cards and coins, not quarks or electrons. The Wikipedia article even says “Schrödinger did not wish to promote the idea of dead-and-live cats as a serious possibility; on the contrary, he intended the example to illustrate the absurdity of the existing view of quantum mechanics”. Cards can’t be and not-be aces until flipped. Get out of here."
  },
  {
    "objectID": "posts/2021-04-03-confidence-intervals/index.html#wrapping-up-dont-me",
    "href": "posts/2021-04-03-confidence-intervals/index.html#wrapping-up-dont-me",
    "title": "On Interpretations of Confidence Intervals",
    "section": "Wrapping Up, Don’t @ Me",
    "text": "Wrapping Up, Don’t @ Me\nTo be completely fair, I think the question about the cards I’ve presented to you is unfair. The question asks for a probability, and while 0 and 1 are valid probabilities, the question is phrased in a way so that you are prompted for a number between 0 and 1. Likewise, the name “95% confidence interval” begs for the wrong interpretation. That is the problem we face when we use language, which is naturally imprecise and full of shortcuts and ambiguity, to talk about things as precise as mathematics. It is a seminal case study in what I like to call the precision-usefulness trade off; precise statements are not useful. It is by, interpreting them and communicating them in common language that they become useful and that usefulness comes at the cost of precision (note, this explanation of the trade off is itself susceptible to the trade off). The important part is that we use confidence intervals to convey uncertainty in the estimate for which they are derived from. It isn’t important what you or I think about it, as the confidence interval is merely a means to an end.\nAS I noted, the mistake is benign, and these arguments are mostly a mental exercise than a fight against a method which may induce harm. Were it not for COVID19, I would encourage us all to go out for a beer and have these conversations rather than do it over twitter. Anyway, if you promise not to @ me anymore about this and I promise not to tweet about it anymore."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html",
    "href": "posts/2021-11-23-bootstrap/index.html",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "",
    "text": "Its late, I can’t sleep, so I’m writing a blog post about the optimism corrected bootstrap.\nIn case you don’t know, epidemiology/biostatistics people working on prediction models like to validate their models in a slightly different way than your run-in-the-mill data scientist. Now, it should be unsurprising that this has generated some discussion between ML people and epi/biostats people, but I’m going to ignore this for now. I’m going to assume you have good reason for wanting to do the optimism corrected bootstrap in python, especially with sklearn, and if you don’t and want to discuss the pros and cons fo the method instead then lalalalalala I can’t hear you."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#the-optimism-corrected-bootstrap-in-7-steps",
    "href": "posts/2021-11-23-bootstrap/index.html#the-optimism-corrected-bootstrap-in-7-steps",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "The Optimism Corrected Bootstrap in 7 Steps",
    "text": "The Optimism Corrected Bootstrap in 7 Steps\nAs a primer, you might want to tread Alex Hayes’ pretty good blog post about variants of the bootstrap for predictive performance. It is more mathy than I care to be right now and in R should that be your thing.\nTo do the optimism corrected bootstrap, follow these 7 steps as found in Ewout W. Steyerberg’s Clinical Prediction Models.\n\nConstruct a model in the original sample; determine the apparent performance on the data from the sample used to construct the model.\nDraw a bootstrap sample (Sample*) with replacement from the original sample.\nConstruct a model (Model) in Sample, replaying every step that was done in the original sample, especially model specification steps such as selection of predictors. Determine the bootstrap performance as the apparent performance of Model* in Sample.\nApply Model* to the original sample without any modification to determine the test performance.\nCalculate the optimism as the difference between bootstrap performance and test performance.\nRepeat steps 1–4 many times, at least 200, to obtain a stable mean estimate of the optimism.\nSubtract the mean optimism estimate (step 6) from the apparent performance (step 1) to obtain the optimism-corrected performance estimate.\n\nThis procedure is very straight forward, and could easily be coded up from scratch, but I want to use as much existing code as I can and put sklearn on my resume, so let’s talk about what tools exist in sklearn to do cross validation and how we could use them to perform these steps."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#cross-validation-in-sklearn",
    "href": "posts/2021-11-23-bootstrap/index.html#cross-validation-in-sklearn",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "Cross Validation in Sklearn",
    "text": "Cross Validation in Sklearn\nWhen you pass arguments like cv=5 in sklearn’s many functions, what you’re really doing is passing 5 to sklearn.model_selection.KFold. See sklearn.model_selection.cross_validate which calls a function called ‘check_cv’ to verify this. KFold.split returns a generator, which when passed to next yields a pair of train and test indicides. The inner workings of KFold might look something like\nfor _ in range(number_folds):\n    train_ix = make_train_ix()\n    test_ix = make_test_ix()\n    yield (trian_ix, test_ix)\nThose incidies are used to slice X and y to do the cross validation. So, if we are going to hack sklearn to do the optimisim corrected bootstrap for us, we really just need to write a generator to give me a bunch of indicies. According to step 2 and 3 above, the train indicies need to be resamples of np.arange(len(X)) (ask yourself “why?”). According to step 4, the test indicies need to be np.arnge(len(X)) (again….”why?“).\nOnce we have a generator to do give us our indicies, we can use sklearn.model_selection.cross_validate to fit models on the resampled data and predict on the original sample (step 4). If we pass return_train_score=True to cross_validate we can get the bootstrap performances as well as the test performances (step 5). All we need to do then is calculate the average difference between the two (step 6) and then add this quantity to the apparent performance we got from step 1.\nThat all sounds very complex, but the code is decieptively simple."
  },
  {
    "objectID": "posts/2021-11-23-bootstrap/index.html#the-code-i-know-you-skipped-here-dont-lie",
    "href": "posts/2021-11-23-bootstrap/index.html#the-code-i-know-you-skipped-here-dont-lie",
    "title": "Hacking Sklearn To Do The Optimism Corrected Bootstrap",
    "section": "The Code (I Know You Skipped Here, Don’t Lie)",
    "text": "The Code (I Know You Skipped Here, Don’t Lie)\n\nimport numpy as np\nfrom numpy.core.fromnumeric import mean\nfrom sklearn.model_selection import cross_validate, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.utils import resample\n\n# Need some data to predict with\ndata = load_diabetes()\nX, y = data['data'], data['target']\n\nclass OptimisimBootstrap():\n\n    def __init__(self, n_bootstraps):\n\n        self.n_bootstraps = n_bootstraps\n\n    def split(self, X, y,*_):\n\n        n = len(X)\n        test_ix = np.arange(n)\n\n        for _ in range(self.n_bootstraps):\n            train_ix = resample(test_ix)\n            yield (train_ix, test_ix)\n\n# Optimism Corrected\nmodel = LinearRegression()\nmodel.fit(X, y)\napparent_performance = mean_squared_error(y, model.predict(X))\n\nopt_cv = OptimisimBootstrap(n_bootstraps=250)\nmse = make_scorer(mean_squared_error)\ncv = cross_validate(model, X, y, cv=opt_cv, scoring=mse, return_train_score=True)\noptimism = cv['test_score'] - cv['train_score']\noptimism_corrected = apparent_performance + optimism.mean()\nprint(f'Optimism Corrected: {optimism_corrected:.2f}')\n\n# Compare against regular cv\ncv = cross_validate(model, X, y, cv = 10, scoring=mse)['test_score'].mean()\nprint(f'regular cv: {cv:.2f}')\n\n# Compare against repeated cv\ncv = cross_validate(model, X, y, cv = RepeatedKFold(n_splits=10, n_repeats=100), scoring=mse)['test_score'].mean()\nprint(f'regular cv: {cv:.2f}')\n\nOptimism Corrected: 2998.79\nregular cv: 3000.38\n\n\nregular cv: 3012.67\n\n\nThe two estimates (optimism corrected and 10 fold) should be reasonably close together, but uh don’t run this code multiple times. You might see that the optimism corrected estimate is quite noisy meaning I’m either wrong or that twitter thread I linked to might have some merit."
  }
]