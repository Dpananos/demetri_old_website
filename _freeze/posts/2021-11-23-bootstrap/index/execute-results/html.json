{
  "hash": "30195750f96894c5fb46f761029a64ae",
  "result": {
    "markdown": "---\ntitle: \"Hacking Sklearn To Do The Optimism Corrected Bootstrap\"\nauthor: \"Demetri Pananos\"\ndate: \"2022-11-23\"\ncategories: [Statistics, Machine Learning, Python, Scikit-Learn]\n---\n\nIts late, I can't sleep, so I'm writing a blog post about the optimism corrected bootstrap.\n\nIn case you don't know, epidemiology/biostatistics people working on prediction models like to validate their models in a slightly different way than your run-in-the-mill data scientist. Now, it should be unsurprising that [this has generated some discussion](https://twitter.com/GaelVaroquaux/status/1293818409197731840) between ML people and epi/biostats people, but I'm going to ignore this for now.  I'm going to assume you have good reason for wanting to do the optimism corrected bootstrap in python, especially with sklearn, and if you don't and want to discuss the pros and cons fo the method instead then lalalalalala I can't hear you.\n\n## The Optimism Corrected Bootstrap in 7 Steps\n\nAs a primer, you might want to tread Alex Hayes' [pretty good blog post about variants of the bootstrap](https://www.alexpghayes.com/blog/predictive-performance-via-bootstrap-variants/) for predictive performance.  It is more mathy than I care to be right now and in R should that be your thing.\n\nTo do the optimism corrected bootstrap, follow these 7 steps as found in [Ewout W. Steyerberg's *Clinical Prediction Models*](https://link.springer.com/book/10.1007/978-0-387-77244-8). \n\n1. Construct a model in the original sample; determine the apparent performance on the data from the sample used to construct the model.\n\n2. Draw a bootstrap sample (Sample*) with replacement from the original sample.\n\n3. Construct a model (Model*) in Sample*, replaying every step that was done in the original sample, especially model specification steps such as selection of predictors. Determine the bootstrap performance as the apparent performance of Model* in Sample.\n\n4. Apply Model* to the original sample without any modification to determine the test performance.\n\n5. Calculate the optimism as the difference between bootstrap performance and test performance.\n\n6. Repeat steps 1â€“4 many times, at least 200, to obtain a stable mean estimate of the optimism.\n\n7. Subtract the mean optimism estimate (step 6) from the apparent performance (step 1) to obtain the optimism-corrected performance estimate.\n\nThis procedure is very straight forward, and could easily be coded up from scratch, but I want to use as much existing code as I can and put sklearn on my resume, so let's talk about what tools exist in sklearn to do cross validation and how we could use them to perform these steps.\n\n## Cross Validation in Sklearn\n\nWhen you pass arguments like `cv=5` in sklearn's many functions, what you're really doing is passing `5` to `sklearn.model_selection.KFold`.  See [`sklearn.model_selection.cross_validate`](https://github.com/scikit-learn/scikit-learn/blob/0d378913b/sklearn/model_selection/_validation.py#L48) which calls a function called ['check_cv'](https://github.com/scikit-learn/scikit-learn/blob/0d378913be6d7e485b792ea36e9268be31ed52d0/sklearn/model_selection/_split.py#L2262) to verify this.  `KFold.split` returns a generator, which when passed to `next` yields a pair of train and test indicides.  The inner workings of `KFold` might look something like\n\n```python\nfor _ in range(number_folds):\n    train_ix = make_train_ix()\n    test_ix = make_test_ix()\n    yield (trian_ix, test_ix)\n```\n\nThose incidies are used to slice `X` and `y` to do the cross validation. So, if we are going to hack sklearn to do the optimisim corrected bootstrap for us, we really just need to write a generator to give me a bunch of indicies.  According to step 2 and 3 above, the train indicies need to be resamples of `np.arange(len(X))` (ask yourself \"why?\"). According to step 4, the test indicies need to be `np.arnge(len(X))` (again....\"why?\").\n\nOnce we have a generator to do give us our indicies, we can use `sklearn.model_selection.cross_validate` to fit models on the resampled data and predict on the original sample (step 4).  If we pass `return_train_score=True` to `cross_validate` we can get the bootstrap performances as well as the test performances (step 5).  All we need to do then is calculate the average difference between the two (step 6) and then add this quantity to the apparent performance we got from step 1.\n\nThat all sounds very complex, but the code is decieptively simple.\n\n\n## The Code (I Know You Skipped Here, Don't Lie)\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom numpy.core.fromnumeric import mean\nfrom sklearn.model_selection import cross_validate, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.utils import resample\n\n# Need some data to predict with\ndata = load_diabetes()\nX, y = data['data'], data['target']\n\nclass OptimisimBootstrap():\n\n    def __init__(self, n_bootstraps):\n\n        self.n_bootstraps = n_bootstraps\n\n    def split(self, X, y,*_):\n\n        n = len(X)\n        test_ix = np.arange(n)\n\n        for _ in range(self.n_bootstraps):\n            train_ix = resample(test_ix)\n            yield (train_ix, test_ix)\n\n# Optimism Corrected\nmodel = LinearRegression()\nmodel.fit(X, y)\napparent_performance = mean_squared_error(y, model.predict(X))\n\nopt_cv = OptimisimBootstrap(n_bootstraps=250)\nmse = make_scorer(mean_squared_error)\ncv = cross_validate(model, X, y, cv=opt_cv, scoring=mse, return_train_score=True)\noptimism = cv['test_score'] - cv['train_score']\noptimism_corrected = apparent_performance + optimism.mean()\nprint(f'Optimism Corrected: {optimism_corrected:.2f}')\n\n# Compare against regular cv\ncv = cross_validate(model, X, y, cv = 10, scoring=mse)['test_score'].mean()\nprint(f'regular cv: {cv:.2f}')\n\n# Compare against repeated cv\ncv = cross_validate(model, X, y, cv = RepeatedKFold(n_splits=10, n_repeats=100), scoring=mse)['test_score'].mean()\nprint(f'regular cv: {cv:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimism Corrected: 2998.79\nregular cv: 3000.38\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nregular cv: 3012.67\n```\n:::\n:::\n\n\nThe two estimates (optimism corrected and 10 fold) should be reasonably close together, but uh don't run this code multiple times.  You might see that the optimism corrected estimate is quite noisy meaning I'm either wrong or that twitter thread I linked to might have some merit.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}